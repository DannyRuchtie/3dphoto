<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="theme-color" content="#000000">
    <link rel="apple-touch-icon" href="icon.png">
    <title>3D Photo Viewer</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.4/dist/tf.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface@0.0.7"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/dat-gui/0.7.9/dat.gui.min.js"></script>
    <style>
       html,body { 
            margin: 0; 
            color: #fff;
            font-family: system-ui, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            background: #000;
            height: 100%;
            width: 100%;
            position: fixed;
            overflow: hidden;
            -webkit-touch-callout: none;
            -webkit-user-select: none;
            user-select: none;
        }
        canvas { display: block; }
        button {
            background: rgba(35, 35, 35, 0.8);
            color: #fff;
            border: 1px solid #1e1e1e;
            padding: 8px 16px;
            border-radius: 8px;
            cursor: pointer;
            margin: 5px 0;
            font-family: inherit;
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
        }
        button:hover {
            background: rgba(45, 45, 45, 0.8);
        }
        input[type="file"] {
            color: #fff;
            margin-bottom: 10px;
            width: 100%;
        }
        .dg.ac {
            position: fixed !important;
            top: 20px !important;
            right: 20px !important;
            margin-right: 20px !important;
        }
        #webcam-container {
            position: absolute;
            top: 20px;
            left: 20px;
            width: 160px;
            height: 120px;
            border-radius: 8px;
            overflow: hidden;
            z-index: 1000;
            transition: opacity 0.3s ease;
        }
        #webcam {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        #tracking-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        #upload-container {
            position: absolute;
            height: 113px;
            bottom: 20px;
            left: 20px;
            background: rgba(23, 23, 23, 0.7);
            border: 1px solid #1e1e1e;
            padding: 20px;
            border-radius: 8px;
            z-index: 1000;
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
        }
        #status {
            margin-top: 10px;
            color: #333;
        }
        #tempCanvas {
            display: none;
        }
        #depthPreview {
            position: absolute;
            bottom: 20px;
            right: 20px;
            background: rgba(23, 23, 23, 0.7);
            border: 1px solid #1e1e1e;
            padding: 20px;
            border-radius: 8px;
            width: 200px;
            height: 115px;
            display: flex;
            align-items: center;
            justify-content: center;
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
        }
        #previewCanvas {
            width: 100%;
            height: auto;
            border-radius: 8px;
        }
        canvas.webgl {
            border-radius: 8px;
        }
        .dg.main {
            background: rgba(23, 23, 23, 0.7) !important;
            backdrop-filter: blur(10px) !important;
            -webkit-backdrop-filter: blur(10px) !important;
        }
        .dg .close-button {
            background-color: rgba(35, 35, 35, 0.8) !important;
        }
        .dg .cr.number input[type=text] {
            background: #1e1e1e !important;
            color: #fff !important;
        }
        @media (display-mode: standalone) {
            body {
                padding-top: env(safe-area-inset-top);
                padding-bottom: env(safe-area-inset-bottom);
            }
        }
    </style>
</head>
<body>
    <div id="upload-container">
        <input type="file" id="imageInput" accept="image/*">
        <button onclick="processImage()">Process & View in 3D</button>
        <div id="status"></div>
    </div>
    <canvas id="tempCanvas"></canvas>
    <div id="depthPreview">

        <canvas id="previewCanvas"></canvas>
    </div>
    <div id="webcam-container">
        <video id="webcam"></video>
        <canvas id="tracking-overlay"></canvas>
    </div>

    <script>
        let scene, camera, renderer;
        let mesh;
        let model;
        let faceDetector;
        let webcam;
        let initialCameraPosition = { x: 0, y: 0, z: 2.5 };
        let controls = {
            xSensitivity: 1.0,
            ySensitivity: 1.0,
            smoothing: 0.1,
            cameraDistance: 2,
            invertCamera: false,
            selectedImage: 'images/example-1.jpeg',
            imageOptions: ['images/example-1.jpeg', 'images/example-2.jpeg', 'images/example-3.jpeg', 'images/example-4.jpeg', 'images/example-5.jpeg'],
            loadImage: function() {
                loadSelectedImage();
            }
        };

        const CORS_PROXY = 'https://cors-anywhere.herokuapp.com/';

        class Pydnet {
            async init() {
                const MODEL = "https://raw.githubusercontent.com/FilippoAleotti/demo_live/master/assets/js/pydnet.json";
                this.model = await tf.loadGraphModel(MODEL);
                this.modelHeight = 384;
                this.modelWidth = 640;
                return this;
            }

            async predict(img) {
                const aspectRatio = img.width / img.height;
                let outputWidth, outputHeight;
                
                if (aspectRatio > 1) {
                    outputWidth = 1920;
                    outputHeight = Math.round(1920 / aspectRatio);
                } else {
                    outputHeight = 1920;
                    outputWidth = Math.round(1920 * aspectRatio);
                }

                const [data, resizeInputData] = tf.tidy(() => {
                    const raw_input = tf.browser.fromPixels(img);
                    
                    const modelInput = tf.image.resizeBilinear(raw_input, [this.modelHeight, this.modelWidth]);
                    const preprocessedInput = modelInput.expandDims();
                    const normalizedInput = tf.div(preprocessedInput, 255.0);
                    
                    const result = this.model.predict(normalizedInput);
                    let processedResult = this.prepareOutput(result);
                    
                    const resizedResult = tf.image.resizeBilinear(
                        processedResult.expandDims(2),
                        [outputHeight, outputWidth],
                        true
                    );
                    
                    const finalInput = tf.image.resizeBilinear(raw_input, [outputHeight, outputWidth]);
                    const finalInputInt = tf.cast(finalInput, "int32");
                    
                    const data = resizedResult.squeeze().dataSync();
                    const resizeInputData = finalInputInt.dataSync();
                    return [data, resizeInputData, outputWidth, outputHeight];
                });
                await tf.nextFrame();
                return [data, resizeInputData, outputWidth, outputHeight];
            }

            prepareOutput(tensor) {
                return tf.tidy(() => {
                    tensor = tf.relu(tensor);
                    tensor = tf.squeeze(tensor);
                    tensor = tf.expandDims(tensor, 2);
                    const gaussianKernel = tf.tensor2d(
                        [
                            [1, 2, 1],
                            [2, 4, 2],
                            [1, 2, 1]
                        ]
                    ).div(16).expandDims(2).expandDims(3);
                    
                    tensor = tf.conv2d(
                        tensor.expandDims(0),
                        gaussianKernel,
                        1,
                        'same'
                    ).squeeze();

                    const min_value = tf.min(tensor);
                    const max_value = tf.max(tensor);
                    const normalized = tf.div(
                        tf.sub(tensor, min_value),
                        tf.sub(max_value, min_value)
                    );
                    return tf.mul(normalized, 255.0);
                });
            }
        }

        async function init() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.domElement.classList.add('webgl');
            document.body.appendChild(renderer.domElement);

            camera.position.z = controls.cameraDistance;
            initialCameraPosition = { x: 0, y: 0, z: controls.cameraDistance };

            const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
            scene.add(ambientLight);

            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.5);
            directionalLight.position.set(5, 5, 5);
            scene.add(directionalLight);

            await setupFaceTracking();
            setupGUI();

            animate();
        }

        async function processImageElement(imageElement) {
            try {
                const [depthData, resizeInputData, width, height] = await model.predict(imageElement);
                
                const tempCanvas = document.getElementById('tempCanvas');
                const previewCanvas = document.getElementById('previewCanvas');
                tempCanvas.width = width;
                tempCanvas.height = height;
                previewCanvas.width = 200;
                previewCanvas.height = Math.round(200 * (height / width));

                const ctx = tempCanvas.getContext('2d');
                const imageData = ctx.createImageData(width, height);
                
                let i = 0;
                for (let y = 0; y < height; y++) {
                    for (let x = 0; x < width; x++) {
                        const index = y * width + x;
                        const depth = depthData[index];
                        imageData.data[i] = depth;
                        imageData.data[i + 1] = depth;
                        imageData.data[i + 2] = depth;
                        imageData.data[i + 3] = 255;
                        i += 4;
                    }
                }
                
                ctx.putImageData(imageData, 0, 0);

                const previewCtx = previewCanvas.getContext('2d');
                previewCtx.drawImage(tempCanvas, 0, 0, previewCanvas.width, previewCanvas.height);

                createDisplacementMesh(imageElement, tempCanvas, width / height);
                return true;
            } catch (error) {
                console.error('Error processing image:', error);
                throw error;
            }
        }

        async function processImage() {
            const fileInput = document.getElementById('imageInput');
            const statusDiv = document.getElementById('status');
            
            if (!fileInput.files.length) {
                statusDiv.textContent = 'Please select an image first';
                return;
            }

            statusDiv.textContent = 'Processing image...';

            const img = new Image();
            img.onload = async function() {
                try {
                    await processImageElement(img);
                    statusDiv.textContent = 'Processing complete!';
                } catch (error) {
                    statusDiv.textContent = 'Error processing image: ' + error.message;
                }
            };
            
            img.src = URL.createObjectURL(fileInput.files[0]);
        }

        function createDisplacementMesh(imageElement, depthCanvas, aspectRatio) {
            const texture = new THREE.Texture(imageElement);
            texture.needsUpdate = true;

            const depthTexture = new THREE.Texture(depthCanvas);
            depthTexture.minFilter = THREE.LinearFilter;
            depthTexture.magFilter = THREE.LinearFilter;
            depthTexture.needsUpdate = true;

            // Create clipping shader
            const vertexShader = `
                varying vec2 vUv;
                varying vec3 vPosition;
                uniform sampler2D displacementMap;
                uniform float displacementScale;
                uniform float displacementBias;
                void main() {
                    vUv = uv;
                    vPosition = position;
                    vec4 displacement = texture2D(displacementMap, uv);
                    vec3 newPosition = position + normal * (displacement.r * displacementScale + displacementBias);
                    gl_Position = projectionMatrix * modelViewMatrix * vec4(newPosition, 1.0);
                }
            `;

            const fragmentShader = `
                uniform sampler2D map;
                uniform sampler2D displacementMap;
                varying vec2 vUv;
                varying vec3 vPosition;
                
                void main() {
                    vec2 uv = vUv;
                    vec4 color = texture2D(map, uv);
                    
                    // Simple rectangle with rounded corners
                    float cornerRadius = 0.02;  // Adjust this for more/less rounding
                    vec2 halfSize = vec2(0.5);
                    vec2 edgeDistance = abs(vUv - 0.5);
                    
                    if (max(edgeDistance.x - (halfSize.x - cornerRadius), 
                           edgeDistance.y - (halfSize.y - cornerRadius)) > 0.0) {
                        // Check if we're outside the rounded rectangle
                        vec2 cornerEdgeDistance = max(edgeDistance - halfSize + cornerRadius, 0.0);
                        if (length(cornerEdgeDistance) > cornerRadius) {
                            discard;
                        }
                    }
                    
                    gl_FragColor = color;
                }
            `;

            if (mesh) {
                scene.remove(mesh);
            }

            const width = 3;
            const height = width / aspectRatio;
            const geometry = new THREE.PlaneGeometry(
                width, 
                height, 
                Math.min(512, Math.round(512 * aspectRatio)),
                Math.min(512, Math.round(512 / aspectRatio))
            );

            const material = new THREE.ShaderMaterial({
                uniforms: {
                    map: { value: texture },
                    displacementMap: { value: depthTexture },
                    displacementScale: { value: 0.15 },
                    displacementBias: { value: -0.15 }
                },
                vertexShader: vertexShader,
                fragmentShader: fragmentShader,
                side: THREE.DoubleSide,
            });

            mesh = new THREE.Mesh(geometry, material);
            scene.add(mesh);

            camera.position.z = Math.max(2.5, 1.5 + aspectRatio);
        }

        function animate() {
            requestAnimationFrame(animate);
            renderer.render(scene, camera);
        }

        // Initialize Three.js scene
        init();

        // Initialize depth estimation model
        async function setupModel() {
            const statusDiv = document.getElementById('status');
            try {
                statusDiv.textContent = 'Loading model...';
                model = await new Pydnet().init();
                statusDiv.textContent = 'Model loaded, loading default image...';
                await loadDefaultImage();
            } catch (error) {
                console.error('Error setting up model:', error);
                statusDiv.textContent = 'Error loading model: ' + error.message;
            }
        }
        setupModel();

        // Handle window resizing
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        async function loadDefaultImage() {
            await loadSelectedImage();
        }

        async function predict(model, image) {
            // Convert image to tensor and normalize
            const tensor = tf.browser.fromPixels(image)
                .resizeBilinear([384, 384]) // MiDaS model expects 384x384 input
                .expandDims(0)
                .toFloat()
                .div(255.0);

            // Run prediction
            try {
                const prediction = await model.predict(tensor);
                
                // Ensure proper axis handling (this is where the error was occurring)
                const depthMap = prediction.squeeze(); // Remove batch dimension
                
                // Normalize depth values to 0-1 range
                const normalizedDepth = depthMap.sub(depthMap.min())
                                                .div(depthMap.max().sub(depthMap.min()));
                
                // Convert to image data
                const depthArray = await normalizedDepth.data();
                const width = depthMap.shape[1];
                const height = depthMap.shape[0];
                
                // Create canvas for depth map
                const canvas = document.createElement('canvas');
                canvas.width = width;
                canvas.height = height;
                const ctx = canvas.getContext('2d');
                const imageData = ctx.createImageData(width, height);
                
                // Fill image data with depth values
                for (let i = 0; i < depthArray.length; i++) {
                    const value = Math.floor(depthArray[i] * 255);
                    imageData.data[i * 4] = value;     // R
                    imageData.data[i * 4 + 1] = value; // G
                    imageData.data[i * 4 + 2] = value; // B
                    imageData.data[i * 4 + 3] = 255;   // A
                }
                
                ctx.putImageData(imageData, 0, 0);
                
                // Clean up tensors
                tensor.dispose();
                prediction.dispose();
                depthMap.dispose();
                normalizedDepth.dispose();
                
                return canvas;
            } catch (error) {
                console.error('Error in depth estimation:', error);
                throw error;
            }
        }

        async function processImage(imageElement) {
            try {
                const model = await tf.loadGraphModel('path/to/your/model.json');
                const depthMap = await predict(model, imageElement);
                
                // Update the depth map preview
                const previewContainer = document.querySelector('.depth-map-preview');
                if (previewContainer) {
                    previewContainer.innerHTML = '';
                    previewContainer.appendChild(depthMap);
                }
                
                // Use the depth map for Three.js visualization
                updateThreeJsScene(imageElement, depthMap);
            } catch (error) {
                console.error('Processing failed:', error);
            }
        }

        function updateThreeJsScene(originalImage, depthMap) {
            // Create texture from depth map
            const depthTexture = new THREE.CanvasTexture(depthMap);
            
            // Create displacement material
            const material = new THREE.MeshStandardMaterial({
                map: new THREE.TextureLoader().load(originalImage.src),
                displacementMap: depthTexture,
                displacementScale: 50, // Adjust this value to control depth effect
                displacementBias: 0
            });

            // Update your mesh with the new material
            // ... rest of your Three.js code
        }

        async function setupFaceTracking() {
            const video = document.getElementById('webcam');
            const overlay = document.getElementById('tracking-overlay');
            overlay.width = 160;
            overlay.height = 120;
            const ctx = overlay.getContext('2d');
            
            // Load BlazeFace model
            faceDetector = await blazeface.load();
            
            // Setup webcam
            video.width = 160;
            video.height = 120;
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    'video': {
                        width: 160,
                        height: 120
                    }
                });
                video.srcObject = stream;
                await video.play();
            } catch (error) {
                console.error('Error accessing webcam:', error);
                return;
            }
            
            // Start detection loop
            detectFaces(ctx);
        }

        async function detectFaces(ctx) {
            const video = document.getElementById('webcam');
            
            async function detect() {
                const faces = await faceDetector.estimateFaces(video, false);
                // Clear previous frame
                ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
                
                if (faces.length > 0) {
                    const face = faces[0];
                    onResults(face);
                    
                    // Draw tracking box
                    ctx.strokeStyle = '#00ff00';
                    ctx.lineWidth = 2;
                    const startX = face.topLeft[0];
                    const startY = face.topLeft[1];
                    const width = face.bottomRight[0] - face.topLeft[0];
                    const height = face.bottomRight[1] - face.topLeft[1];
                    ctx.strokeRect(startX, startY, width, height);
                    
                    // Draw tracking info
                    ctx.fillStyle = 'white';
                    ctx.font = '10px Arial';
                    ctx.fillText(`X: ${Math.round(startX)}`, 5, 15);
                    ctx.fillText(`Y: ${Math.round(startY)}`, 5, 30);
                    ctx.fillText(`Size: ${Math.round(width)}x${Math.round(height)}`, 5, 45);
                }
                requestAnimationFrame(detect);
            }
            
            detect();
        }

        function onResults(face) {
            if (!face || !mesh) return;
            
            const centerX = face.topLeft[0] / 160;
            const centerY = face.topLeft[1] / 120;
            
            // Calculate offsets with adjustable sensitivity and direction
            const direction = controls.invertCamera ? -1 : 1;
            const xOffset = direction * (centerX - 0.5) * controls.xSensitivity;
            const yOffset = direction * (centerY - 0.5) * controls.ySensitivity;
            
            // Calculate target camera position
            const targetX = initialCameraPosition.x + xOffset;
            const targetY = initialCameraPosition.y + yOffset;
            const targetZ = controls.cameraDistance;
            
            // Smooth camera movement with adjustable lerp
            camera.position.x += (targetX - camera.position.x) * controls.smoothing;
            camera.position.y += (targetY - camera.position.y) * controls.smoothing;
            camera.position.z = targetZ;
            
            camera.lookAt(mesh.position);
        }

        function setupGUI() {
            const gui = new dat.GUI();
            const trackingFolder = gui.addFolder('Face Tracking Controls');
            trackingFolder.add(controls, 'xSensitivity', 0.1, 3.0).name('X Sensitivity');
            trackingFolder.add(controls, 'ySensitivity', 0.1, 3.0).name('Y Sensitivity');
            trackingFolder.add(controls, 'smoothing', 0.01, 0.5).name('Smoothing');
            trackingFolder.add(controls, 'cameraDistance', 0.5, 3.0).name('Camera Distance')
                .onChange((value) => {
                    initialCameraPosition.z = value;
                });
            trackingFolder.add(controls, 'invertCamera').name('Invert Camera Movement');
            
            const imageFolder = gui.addFolder('Image Controls');
            imageFolder.add(controls, 'selectedImage', controls.imageOptions).name('Select Image');
            imageFolder.add(controls, 'loadImage').name('Load Selected Image');
            
            trackingFolder.open();
            imageFolder.open();
        }

        async function loadSelectedImage() {
            const img = new Image();
            img.crossOrigin = "Anonymous";
            
            img.onload = async function() {
                try {
                    await processImageElement(img);
                    document.getElementById('status').textContent = 'Image loaded and processed!';
                } catch (error) {
                    console.error('Error processing image:', error);
                    document.getElementById('status').textContent = 'Error processing image';
                }
            };
            
            img.src = controls.selectedImage;
        }
    </script>
</body>
</html> 